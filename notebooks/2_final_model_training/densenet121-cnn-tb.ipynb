{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Final Model Training for DenseNet121\n",
    "\n",
    "This notebook takes the optimal hyperparameters found via the Bayesian Optimization study in `../1_bayesian_optimization/bo-densenet121.ipynb`, builds the final model, trains it, and evaluates its performance on the hold-out test set."
   ],
   "id": "327319606ecbe210"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setup and Imports",
   "id": "d40d4dccddee19a7"
  },
  {
   "cell_type": "code",
   "id": "af49e5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:45:37.387112Z",
     "iopub.status.busy": "2025-05-18T07:45:37.386833Z",
     "iopub.status.idle": "2025-05-18T07:45:52.827263Z",
     "shell.execute_reply": "2025-05-18T07:45:52.826464Z"
    },
    "papermill": {
     "duration": 15.444919,
     "end_time": "2025-05-18T07:45:52.828596",
     "exception": false,
     "start_time": "2025-05-18T07:45:37.383677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# determinism\n",
    "SEED = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ],
   "id": "9bbf051350130e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6ccb2a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:45:52.840073Z",
     "iopub.status.busy": "2025-05-18T07:45:52.839577Z",
     "iopub.status.idle": "2025-05-18T07:45:52.845157Z",
     "shell.execute_reply": "2025-05-18T07:45:52.844484Z"
    },
    "papermill": {
     "duration": 0.014089,
     "end_time": "2025-05-18T07:45:52.846361",
     "exception": false,
     "start_time": "2025-05-18T07:45:52.832272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parameters\n",
    "TARGET_MODALITY = 'tb'\n",
    "\n",
    "PRETRAINED_WEIGHTS_NAME = 'radimagenet'\n",
    "\n",
    "RADIMAGENET_WEIGHT_PATH = '../../weights/RadImageNet-DenseNet121_notop.h5'\n",
    "\n",
    "UNFREEZE_AT_BLOCK = 'conv4_block1_concat'\n",
    "INITIAL_LEARNING_RATE = 1e-4 # LR used if base is frozen OR for top layers\n",
    "FINE_TUNE_LR = 1e-5 # Use a smaller learning rate for fine-tuning if base is unfrozen\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 2\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "TEST_SPLIT = 0.15\n",
    "VALID_SPLIT = 0.15\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Tuberculosis Dataset Paths\n",
    "TB_DATASET_DIR_NAME = 'tuberculosis-tb-chest-xray-dataset'\n",
    "TB_SUBDIR = 'TB_Chest_Radiography_Database'\n",
    "TB_BASE_PATH = os.path.join('../../data/', TB_DATASET_DIR_NAME, TB_SUBDIR)\n",
    "TB_NORMAL_DIR = os.path.join(TB_BASE_PATH, 'Normal/')\n",
    "TB_TUBERCULOSIS_DIR = os.path.join(TB_BASE_PATH, 'Tuberculosis/')\n",
    "\n",
    "BEST_PARAMS_FILE = '../../results/best_params_tb_DenseNet121_radimagenet.json'\n",
    "MODEL_SAVE_PATH = '../../models/densenet121_final_model.weights.h5'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Data Loading and Preparation",
   "id": "c2e4655870cf4b4b"
  },
  {
   "cell_type": "code",
   "id": "43d7ffb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:45:52.853334Z",
     "iopub.status.busy": "2025-05-18T07:45:52.853080Z",
     "iopub.status.idle": "2025-05-18T07:45:52.864529Z",
     "shell.execute_reply": "2025-05-18T07:45:52.864048Z"
    },
    "papermill": {
     "duration": 0.015977,
     "end_time": "2025-05-18T07:45:52.865500",
     "exception": false,
     "start_time": "2025-05-18T07:45:52.849523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def load_and_prepare_data(target_modality):\n",
    "    df_target_modality = pd.DataFrame()\n",
    "\n",
    "    # Load Tuberculosis Data if targeted\n",
    "    if target_modality == 'tb':\n",
    "        print(f\"Loading Tuberculosis data...\")\n",
    "        tb_data_list = []\n",
    "        try:\n",
    "            normal_files = glob(os.path.join(TB_NORMAL_DIR, '*.png'))\n",
    "            for f in normal_files: tb_data_list.append({'filepath': f, 'label': 0})\n",
    "            tuberculosis_files = glob(os.path.join(TB_TUBERCULOSIS_DIR, '*.png'))\n",
    "            for f in tuberculosis_files: tb_data_list.append({'filepath': f, 'label': 1})\n",
    "\n",
    "            if tb_data_list:\n",
    "                df_target_modality = pd.DataFrame(tb_data_list)\n",
    "                print(f\"  Processed {len(df_target_modality)} TB images.\")\n",
    "            else:\n",
    "                print(\"  WARNING: No TB images found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR loading TB data: {e}. Check paths.\")\n",
    "\n",
    "\n",
    "    # Validate Paths and Split\n",
    "    if df_target_modality.empty:\n",
    "        raise ValueError(f\"CRITICAL: No data loaded for target modality '{target_modality}'.\")\n",
    "\n",
    "    # Check file existence\n",
    "    original_count = len(df_target_modality)\n",
    "    df_target_modality = df_target_modality[df_target_modality['filepath'].apply(os.path.exists)]\n",
    "    files_removed = original_count - len(df_target_modality)\n",
    "    if files_removed > 0: print(f\"WARNING: Removed {files_removed} non-existent file entries.\")\n",
    "    print(f\"\\nTotal valid entries for '{target_modality}': {len(df_target_modality)}\")\n",
    "    if len(df_target_modality) == 0: raise ValueError(\"CRITICAL: No valid image files found.\")\n",
    "\n",
    "    df_target_modality['label'] = df_target_modality['label'].astype(int)\n",
    "    print(f\"\\nLabel distribution for '{target_modality}':\\n\", df_target_modality['label'].value_counts())\n",
    "    if df_target_modality['label'].nunique() < 2: raise ValueError(\"CRITICAL: Only one class found in the target modality dataset.\")\n",
    "\n",
    "    # Stratified Split based on Label\n",
    "    train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    try:\n",
    "        # Split off test set\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            df_target_modality,\n",
    "            test_size=TEST_SPLIT,\n",
    "            stratify=df_target_modality['label'],\n",
    "            random_state=SEED\n",
    "        )\n",
    "        # Adjust validation split percentage\n",
    "        val_split_adjusted = VALID_SPLIT / (1 - TEST_SPLIT) if (1 - TEST_SPLIT) > 0 else 0\n",
    "\n",
    "        # Split train_val into train and validation\n",
    "        if val_split_adjusted > 0 and len(train_val_df) > 1 and train_val_df['label'].nunique() > 1:\n",
    "             train_df, val_df = train_test_split(\n",
    "                 train_val_df,\n",
    "                 test_size=val_split_adjusted,\n",
    "                 stratify=train_val_df['label'],\n",
    "                 random_state=SEED\n",
    "             )\n",
    "        elif len(train_val_df) > 0:\n",
    "             train_df = train_val_df\n",
    "             val_df = pd.DataFrame(columns=train_val_df.columns) # Empty val set if split fails\n",
    "             print(\"Warning: Could not perform validation split properly, validation set might be empty or small.\")\n",
    "        else: # train_val_df is empty\n",
    "             train_df = pd.DataFrame(columns=df_target_modality.columns)\n",
    "             val_df = pd.DataFrame(columns=df_target_modality.columns)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nCRITICAL ERROR during split: {e}. Check class distribution and split sizes.\")\n",
    "        raise e # Reraise the exception to stop execution\n",
    "    except Exception as e_gen:\n",
    "         print(f\"\\nUNEXPECTED ERROR during split: {e_gen}\")\n",
    "         raise e_gen\n",
    "\n",
    "    print(\"\\n--- Data Split Summary ---\")\n",
    "    print(f\"Modality: {target_modality}\")\n",
    "    print(f\"Train:      {len(train_df)}\")\n",
    "    print(f\"Validation: {len(val_df)}\")\n",
    "    print(f\"Test:       {len(test_df)}\")\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "    if len(train_df) == 0 or len(val_df) == 0:\n",
    "         print(\"\\nWARNING: Training or validation set is empty after splitting.\")\n",
    "\n",
    "    # Convert labels to string for flow_from_dataframe binary mode\n",
    "    train_df['label'] = train_df['label'].astype(str)\n",
    "    val_df['label'] = val_df['label'].astype(str)\n",
    "    test_df['label'] = test_df['label'].astype(str)\n",
    "\n",
    "\n",
    "    return train_df, val_df, test_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bdda308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:45:52.871216Z",
     "iopub.status.busy": "2025-05-18T07:45:52.870995Z",
     "iopub.status.idle": "2025-05-18T07:46:02.652488Z",
     "shell.execute_reply": "2025-05-18T07:46:02.651652Z"
    },
    "papermill": {
     "duration": 9.785612,
     "end_time": "2025-05-18T07:46:02.653628",
     "exception": false,
     "start_time": "2025-05-18T07:45:52.868016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data Trigger\n",
    "train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "try:\n",
    "    train_df, val_df, test_df = load_and_prepare_data(TARGET_MODALITY)\n",
    "    \n",
    "    print(f\"\\nData loading and splitting for {TARGET_MODALITY} completed.\")\n",
    "\n",
    "    if train_df.empty and val_df.empty and test_df.empty:\n",
    "         print(\"\\nWARNING: All data splits (train, val, test) are empty. This might be due to filtering or data issues.\")\n",
    "    elif train_df.empty or val_df.empty:\n",
    "        print(\"\\nWARNING: Training or validation set is empty after splitting. Training cannot proceed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCRITICAL ERROR during data loading/processing: {e}. Stopping.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Data Augmentation and Generators",
   "id": "b57c3b7f54aefc0f"
  },
  {
   "cell_type": "code",
   "id": "b3a387b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:46:02.659734Z",
     "iopub.status.busy": "2025-05-18T07:46:02.659513Z",
     "iopub.status.idle": "2025-05-18T07:46:04.232413Z",
     "shell.execute_reply": "2025-05-18T07:46:04.231704Z"
    },
    "papermill": {
     "duration": 1.577225,
     "end_time": "2025-05-18T07:46:04.233584",
     "exception": false,
     "start_time": "2025-05-18T07:46:02.656359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Generators\n",
    "\n",
    "train_generator = None\n",
    "validation_generator = None\n",
    "test_generator = None\n",
    "\n",
    "if not train_df.empty and not val_df.empty:\n",
    "\n",
    "    # Augmentation settings for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=densenet_preprocess_input,\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # No augmentation for validation/testing\n",
    "    val_test_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=densenet_preprocess_input\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCreating training generator for '{TARGET_MODALITY}'...\")\n",
    "    try:\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            x_col='filepath',\n",
    "            y_col='label',\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='binary',\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(f\"Training generator created: {len(train_generator)} batches.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating training generator: {e}\")\n",
    "\n",
    "    print(f\"\\nCreating validation generator for '{TARGET_MODALITY}'...\")\n",
    "    try:\n",
    "        validation_generator = val_test_datagen.flow_from_dataframe(\n",
    "            dataframe=val_df,\n",
    "            x_col='filepath',\n",
    "            y_col='label',\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='binary',\n",
    "            shuffle=False\n",
    "        )\n",
    "        print(f\"Validation generator created: {len(validation_generator)} batches.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating validation generator: {e}\")\n",
    "\n",
    "    if not test_df.empty:\n",
    "        print(f\"\\nCreating test generator for '{TARGET_MODALITY}'...\")\n",
    "        try:\n",
    "            test_generator = val_test_datagen.flow_from_dataframe(\n",
    "                dataframe=test_df,\n",
    "                x_col='filepath',\n",
    "                y_col='label',\n",
    "                target_size=IMG_SIZE,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                class_mode='binary',\n",
    "                shuffle=False\n",
    "            )\n",
    "            print(f\"Test generator created: {len(test_generator)} batches.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating test generator: {e}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: Test data DataFrame is empty. Cannot create test generator.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nWARNING: Training or validation DataFrame is empty. Cannot create generators.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Generator Status ---\")\n",
    "print(f\"Train Generator:      {'Created' if train_generator else 'Failed/Skipped'}\")\n",
    "print(f\"Validation Generator: {'Created' if validation_generator else 'Failed/Skipped'}\")\n",
    "print(f\"Test Generator:       {'Created' if test_generator else 'Failed/Skipped'}\")\n",
    "print(\"------------------------\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Load Optimal Hyperparameters",
   "id": "b33544dd6e8a003c"
  },
  {
   "cell_type": "code",
   "id": "0ba619b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:46:04.241835Z",
     "iopub.status.busy": "2025-05-18T07:46:04.241597Z",
     "iopub.status.idle": "2025-05-18T07:46:04.254215Z",
     "shell.execute_reply": "2025-05-18T07:46:04.253509Z"
    },
    "papermill": {
     "duration": 0.017411,
     "end_time": "2025-05-18T07:46:04.255377",
     "exception": false,
     "start_time": "2025-05-18T07:46:04.237966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Best Hyperparameters\n",
    "try:\n",
    "    with open(BEST_PARAMS_FILE, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    print(\"Successfully loaded best hyperparameters:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    hp_filters1 = best_params.get('filters1')\n",
    "    kernel1_str = best_params.get('kernel1')\n",
    "    hp_kernel1 = (int(kernel1_str.split('x')[0]), int(kernel1_str.split('x')[1])) #parse string properly into (x, x)\n",
    "    hp_pool1 = best_params.get('pool1')\n",
    "    hp_filters2 = best_params.get('filters2')\n",
    "    kernel2_str = best_params.get('kernel2')\n",
    "    hp_kernel2 = (int(kernel2_str.split('x')[0]), int(kernel2_str.split('x')[1])) #parse string properly into (x, x)\n",
    "    hp_pool2 = best_params.get('pool2')\n",
    "    hp_filters3 = best_params.get('filters3')\n",
    "    kernel3_str = best_params.get('kernel3')\n",
    "    hp_kernel3 = (int(kernel3_str.split('x')[0]), int(kernel3_str.split('x')[1])) #parse string properly into (x, x)\n",
    "    hp_pool3 = best_params.get('pool3')\n",
    "\n",
    "    # Add checks to ensure keys exist after loading\n",
    "    required_keys = ['filters1', 'kernel1', 'pool1', 'filters2', 'kernel2', 'pool2', 'filters3', 'kernel3', 'pool3']\n",
    "    if not all(key in best_params for key in required_keys):\n",
    "         missing_keys = [key for key in required_keys if key not in best_params]\n",
    "         error_message = f\"ERROR: Hyperparameter file is missing required keys: {missing_keys}\"\n",
    "         print(error_message)\n",
    "         raise ValueError(error_message) # Raise ValueError for missing keys\n",
    "\n",
    "except FileNotFoundError:\n",
    "    error_message = f\"CRITICAL ERROR: Best hyperparameters file not found at {BEST_PARAMS_FILE}. Please ensure the file exists and the path is correct. Stopping execution.\"\n",
    "    print(error_message)\n",
    "    raise SystemExit(error_message)\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "     error_message = f\"CRITICAL ERROR: Could not decode JSON from {BEST_PARAMS_FILE}. Error: {e}. Stopping execution.\"\n",
    "     print(error_message)\n",
    "     raise SystemExit(error_message)\n",
    "\n",
    "except Exception as e:\n",
    "    error_message = f\"CRITICAL ERROR: An unexpected error occurred while loading or processing the hyperparameters file: {e}. Stopping execution.\"\n",
    "    print(error_message)\n",
    "    raise SystemExit(error_message)\n",
    "\n",
    "print(\"\\n--- Hyperparameters loaded and Training Configuration set ---\")\n",
    "print(f\"Filters: {hp_filters1}, {hp_filters2}, {hp_filters3}\")\n",
    "print(f\"Kernels: {hp_kernel1}, {hp_kernel2}, {hp_kernel3}\")\n",
    "print(f\"Pooling: {hp_pool1}, {hp_pool2}, {hp_pool3}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Build Final Model",
   "id": "efaf9e1304a865f"
  },
  {
   "cell_type": "code",
   "id": "6a57aa53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:46:04.262778Z",
     "iopub.status.busy": "2025-05-18T07:46:04.262555Z",
     "iopub.status.idle": "2025-05-18T07:46:04.277716Z",
     "shell.execute_reply": "2025-05-18T07:46:04.277030Z"
    },
    "papermill": {
     "duration": 0.019999,
     "end_time": "2025-05-18T07:46:04.278800",
     "exception": false,
     "start_time": "2025-05-18T07:46:04.258801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Building Function (DenseNet121 Base)\n",
    "def build_model(hp_filters1, hp_kernel1, hp_pool1,\n",
    "                hp_filters2, hp_kernel2, hp_pool2,\n",
    "                hp_filters3, hp_kernel3, hp_pool3,\n",
    "                initial_learning_rate=1e-4,\n",
    "                pretrained_weights_name=None,\n",
    "                radimagenet_path=None,\n",
    "                unfreeze_at_block=None,\n",
    "                fine_tune_lr=1e-5):\n",
    "\n",
    "    # Ensure kernel sizes are tuples\n",
    "    kernel1_tuple = tuple(hp_kernel1) if isinstance(hp_kernel1, list) else hp_kernel1\n",
    "    kernel2_tuple = tuple(hp_kernel2) if isinstance(hp_kernel2, list) else hp_kernel2\n",
    "    kernel3_tuple = tuple(hp_kernel3) if isinstance(hp_kernel3, list) else hp_kernel3\n",
    "\n",
    "    # Base Model Instantiation & Weight Loading\n",
    "    weights_to_load = None\n",
    "    custom_path = None\n",
    "    train_base_initially = False\n",
    "\n",
    "    if pretrained_weights_name == 'imagenet':\n",
    "        print(\"Loading base DenseNet121 model with ImageNet weights...\")\n",
    "        weights_to_load = 'imagenet'\n",
    "    elif pretrained_weights_name == 'radimagenet':\n",
    "        print(\"Preparing to load RadImageNet weights for DenseNet121...\")\n",
    "        weights_to_load = None\n",
    "        custom_path = radimagenet_path\n",
    "    else:\n",
    "        print(\"WARNING: No valid pre-trained weights specified. Initializing DenseNet121 randomly.\")\n",
    "        weights_to_load = None\n",
    "        train_base_initially = True\n",
    "\n",
    "    # Load base DenseNet121 structure\n",
    "    base_model = tf.keras.applications.DenseNet121(\n",
    "        weights=weights_to_load,\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "    )\n",
    "\n",
    "    # Load custom weights if applicable\n",
    "    if custom_path is not None:\n",
    "        if os.path.exists(custom_path):\n",
    "            print(f\"Loading custom weights from: {custom_path}\")\n",
    "            try:\n",
    "                base_model.load_weights(custom_path, by_name=True, skip_mismatch=True)\n",
    "                print(\"Custom weights loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR loading custom weights from {custom_path}: {e}\")\n",
    "                print(\"Proceeding with randomly initialized base model weights.\")\n",
    "                train_base_initially = True\n",
    "        else:\n",
    "            print(f\"WARNING: Custom weight file not found at {custom_path}. Proceeding with randomly initialized base model weights.\")\n",
    "            train_base_initially = True\n",
    "\n",
    "    # Layer Freezing/Unfreezing for Fine-Tuning\n",
    "    if unfreeze_at_block and not train_base_initially:\n",
    "        print(f\"Fine-tuning enabled: Unfreezing layers from layer name starting with '{unfreeze_at_block}' onwards.\")\n",
    "        base_model.trainable = True\n",
    "        unfreeze_layer_index = -1\n",
    "        for i, layer in enumerate(base_model.layers):\n",
    "            if layer.name.startswith(unfreeze_at_block):\n",
    "                unfreeze_layer_index = i\n",
    "                break\n",
    "\n",
    "        if unfreeze_layer_index == -1:\n",
    "            print(f\"WARNING: Layer name prefix '{unfreeze_at_block}' not found. Keeping base frozen.\")\n",
    "            base_model.trainable = False\n",
    "        else:\n",
    "            print(f\"Freezing layers up to index {unfreeze_layer_index}.\")\n",
    "            for layer in base_model.layers[:unfreeze_layer_index]:\n",
    "                layer.trainable = False\n",
    "            print(f\"Layers from index {unfreeze_layer_index} onwards are trainable.\")\n",
    "    else:\n",
    "        print(\"Keeping the base model frozen (or training from scratch if randomly initialized).\")\n",
    "        base_model.trainable = train_base_initially\n",
    "\n",
    "    # Build the full model\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3), name='input_image')\n",
    "\n",
    "    x = base_model(inputs, training=base_model.trainable)\n",
    "\n",
    "    # Custom top layers\n",
    "    pool_func1 = tf.keras.layers.MaxPooling2D if hp_pool1 == 'max' else tf.keras.layers.AveragePooling2D\n",
    "    x = tf.keras.layers.Conv2D(filters=hp_filters1, kernel_size=kernel1_tuple, padding='same', activation='relu', name='custom_block1_conv')(x)\n",
    "    if x.shape[1] is not None and x.shape[1] > 1 and x.shape[2] is not None and x.shape[2] > 1:\n",
    "         x = pool_func1(pool_size=(2, 2), strides=2, name='custom_block1_pool')(x)\n",
    "    else: print(f\"Skipping Pooling Layer 1 (Input shape: {x.shape})\")\n",
    "\n",
    "    pool_func2 = tf.keras.layers.MaxPooling2D if hp_pool2 == 'max' else tf.keras.layers.AveragePooling2D\n",
    "    x = tf.keras.layers.Conv2D(filters=hp_filters2, kernel_size=kernel2_tuple, padding='same', activation='relu', name='custom_block2_conv')(x)\n",
    "    if x.shape[1] is not None and x.shape[1] > 1 and x.shape[2] is not None and x.shape[2] > 1:\n",
    "         x = pool_func2(pool_size=(2, 2), strides=2, name='custom_block2_pool')(x)\n",
    "    else: print(f\"Skipping Pooling Layer 2 (Input shape: {x.shape})\")\n",
    "\n",
    "    pool_func3 = tf.keras.layers.MaxPooling2D if hp_pool3 == 'max' else tf.keras.layers.AveragePooling2D\n",
    "    x = tf.keras.layers.Conv2D(filters=hp_filters3, kernel_size=kernel3_tuple, padding='same', activation='relu', name='custom_block3_conv')(x)\n",
    "    if x.shape[1] is not None and x.shape[1] > 1 and x.shape[2] is not None and x.shape[2] > 1:\n",
    "        x = pool_func3(pool_size=(2, 2), strides=2, name='custom_block3_pool')(x)\n",
    "    else: print(f\"Skipping Pooling Layer 3 (Input shape: {x.shape})\")\n",
    "\n",
    "    # Classification Head\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='custom_gap')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', name='custom_dense')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='custom_output')(x)\n",
    "    # --------------------------\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    lr_to_use = fine_tune_lr if unfreeze_at_block and not train_base_initially else initial_learning_rate\n",
    "    print(f\"Compiling model with learning rate: {lr_to_use}\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_to_use),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "184cf6ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:46:04.285603Z",
     "iopub.status.busy": "2025-05-18T07:46:04.285197Z",
     "iopub.status.idle": "2025-05-18T07:46:09.439914Z",
     "shell.execute_reply": "2025-05-18T07:46:09.439127Z"
    },
    "papermill": {
     "duration": 5.159337,
     "end_time": "2025-05-18T07:46:09.441009",
     "exception": false,
     "start_time": "2025-05-18T07:46:04.281672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build the model using loaded hyperparameters and specified settings\n",
    "model = build_model(\n",
    "    hp_filters1, hp_kernel1, hp_pool1,\n",
    "    hp_filters2, hp_kernel2, hp_pool2,\n",
    "    hp_filters3, hp_kernel3, hp_pool3,\n",
    "    initial_learning_rate=INITIAL_LEARNING_RATE,\n",
    "    pretrained_weights_name=PRETRAINED_WEIGHTS_NAME,\n",
    "    radimagenet_path=RADIMAGENET_WEIGHT_PATH,\n",
    "    unfreeze_at_block=UNFREEZE_AT_BLOCK,\n",
    "    fine_tune_lr=FINE_TUNE_LR\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Print trainable status of layers for verification\n",
    "print(\"\\nTrainable status of base model layers (showing first 10 and last 10):\")\n",
    "if hasattr(model, 'layers') and len(model.layers) > 2 and hasattr(model.layers[2], 'layers'): # Check structure (Input -> Lambda -> Base)\n",
    "     base_layers = model.layers[2].layers # Assumes base model is layer 2\n",
    "     for layer in base_layers[:10]:\n",
    "         print(f\"{layer.name}: {layer.trainable}\")\n",
    "     print(\"...\")\n",
    "     for layer in base_layers[-10:]:\n",
    "         print(f\"{layer.name}: {layer.trainable}\")\n",
    "else:\n",
    "     print(\"Could not display base layer trainable status (model structure might differ).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Define Training Callbacks",
   "id": "c9c06a305e34e9a3"
  },
  {
   "cell_type": "code",
   "id": "31ca90dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:46:09.450251Z",
     "iopub.status.busy": "2025-05-18T07:46:09.450013Z",
     "iopub.status.idle": "2025-05-18T07:46:09.454612Z",
     "shell.execute_reply": "2025-05-18T07:46:09.453923Z"
    },
    "papermill": {
     "duration": 0.010446,
     "end_time": "2025-05-18T07:46:09.455856",
     "exception": false,
     "start_time": "2025-05-18T07:46:09.445410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Callbacks for training\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=EARLY_STOPPING_PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_SAVE_PATH,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=REDUCE_LR_PATIENCE,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_list = [early_stopping, model_checkpoint, reduce_lr]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Train the Model",
   "id": "e0c096b73471bde5"
  },
  {
   "cell_type": "code",
   "id": "49c2c6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:46:09.464814Z",
     "iopub.status.busy": "2025-05-18T07:46:09.464309Z",
     "iopub.status.idle": "2025-05-18T08:18:22.898625Z",
     "shell.execute_reply": "2025-05-18T08:18:22.897862Z"
    },
    "papermill": {
     "duration": 1933.439956,
     "end_time": "2025-05-18T08:18:22.899788",
     "exception": false,
     "start_time": "2025-05-18T07:46:09.459832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check if generators are valid before training\n",
    "if train_generator and validation_generator and len(train_generator) > 0 and len(validation_generator) > 0:\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\n--- Model Training Finished ---\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def plot_history(history):\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Training or Validation generator is invalid. Cannot train model.\")\n",
    "    if not train_generator or len(train_generator) == 0:\n",
    "         print(\"Reason: Training generator issue.\")\n",
    "    if not validation_generator or len(validation_generator) == 0:\n",
    "         print(\"Reason: Validation generator issue.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Final Evaluation on Test Set",
   "id": "248294959f7d5b93"
  },
  {
   "cell_type": "code",
   "id": "477bd34e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:18:23.157850Z",
     "iopub.status.busy": "2025-05-18T08:18:23.157538Z",
     "iopub.status.idle": "2025-05-18T08:19:29.268776Z",
     "shell.execute_reply": "2025-05-18T08:19:29.268045Z"
    },
    "papermill": {
     "duration": 66.373651,
     "end_time": "2025-05-18T08:19:29.400356",
     "exception": false,
     "start_time": "2025-05-18T08:18:23.026705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "model_to_load_weights_from = MODEL_SAVE_PATH\n",
    "\n",
    "# Check if the weight file exists\n",
    "if os.path.exists(model_to_load_weights_from):\n",
    "    print(f\"--- Found saved weights at: {model_to_load_weights_from} ---\")\n",
    "\n",
    "    print(\"Re-building model architecture...\")\n",
    "    model_for_evaluation = build_model(\n",
    "        hp_filters1, hp_kernel1, hp_pool1,\n",
    "        hp_filters2, hp_kernel2, hp_pool2,\n",
    "        hp_filters3, hp_kernel3, hp_pool3,\n",
    "        initial_learning_rate=INITIAL_LEARNING_RATE,\n",
    "        pretrained_weights_name=PRETRAINED_WEIGHTS_NAME,\n",
    "        radimagenet_path=RADIMAGENET_WEIGHT_PATH,\n",
    "        unfreeze_at_block=UNFREEZE_AT_BLOCK,\n",
    "        fine_tune_lr=FINE_TUNE_LR\n",
    "    )\n",
    "    print(\"Model architecture re-built.\")\n",
    "\n",
    "    print(f\"\\n--- Loading Weights into model from {model_to_load_weights_from} ---\")\n",
    "    try:\n",
    "        model_for_evaluation.load_weights(model_to_load_weights_from)\n",
    "        print(\"Weights loaded successfully into the model.\")\n",
    "\n",
    "        # Check if test_generator exists and is valid\n",
    "        if 'test_generator' not in locals() or test_generator is None or len(test_generator) == 0:\n",
    "             print(\"\\nERROR: 'test_generator' object not found or is empty.\")\n",
    "             print(\"Please ensure data loading/preparation cells have been run and test data exists.\")\n",
    "             raise NameError(\"test_generator not defined or empty\")\n",
    "\n",
    "        print(\"\\n--- Evaluating on Test Set (using model.evaluate) ---\")\n",
    "        test_loss, test_acc, test_prec, test_rec = model_for_evaluation.evaluate(\n",
    "            test_generator,\n",
    "            verbose=1\n",
    "        )\n",
    "        test_f1 = 2 * (test_prec * test_rec) / (test_prec + test_rec) if (test_prec + test_rec) > 0 else 0\n",
    "\n",
    "        print(f\"\\nTest Metrics (from evaluate):\")\n",
    "        print(f\"  Loss:      {test_loss:.4f}\")\n",
    "        print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "        print(f\"  Precision: {test_prec:.4f}\")\n",
    "        print(f\"  Recall:    {test_rec:.4f}\")\n",
    "        print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "\n",
    "        # Sklearn Metrics evaluation\n",
    "        print(\"\\nCalculating predictions and true labels batch-by-batch for sklearn metrics...\")\n",
    "        y_true_list = []\n",
    "        y_pred_proba_list = []\n",
    "        num_batches = len(test_generator)\n",
    "        test_generator.reset()\n",
    "        for i in range(num_batches):\n",
    "            try:\n",
    "                x_batch, y_batch = next(test_generator)\n",
    "                if x_batch.shape[0] == 0: continue\n",
    "                y_batch_pred_proba = model_for_evaluation.predict_on_batch(x_batch)\n",
    "                y_true_list.append(y_batch)\n",
    "                y_pred_proba_list.append(y_batch_pred_proba)\n",
    "            except StopIteration: break\n",
    "            except Exception as batch_err:\n",
    "                print(f\"Error processing batch {i+1}: {batch_err}\")\n",
    "                continue\n",
    "        \n",
    "        if not y_true_list:\n",
    "            print(\"ERROR: No data collected from test generator for sklearn metrics.\")\n",
    "        else:\n",
    "            y_true = np.concatenate(y_true_list)\n",
    "            y_pred_proba = np.concatenate(y_pred_proba_list)\n",
    "            min_len = min(len(y_true), len(y_pred_proba))\n",
    "            y_true = y_true[:min_len].astype(int)\n",
    "            y_pred_proba = y_pred_proba[:min_len]\n",
    "            y_pred_classes = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "            sk_acc = accuracy_score(y_true, y_pred_classes)\n",
    "            sk_prec = precision_score(y_true, y_pred_classes, zero_division=0)\n",
    "            sk_rec = recall_score(y_true, y_pred_classes, zero_division=0)\n",
    "            sk_f1 = f1_score(y_true, y_pred_classes, zero_division=0)\n",
    "            print(\"\\nSklearn Metrics:\")\n",
    "            print(f\"  Accuracy:  {sk_acc:.4f}\")\n",
    "            print(f\"  Precision: {sk_prec:.4f}\")\n",
    "            print(f\"  Recall:    {sk_rec:.4f}\")\n",
    "            print(f\"  F1-Score:  {sk_f1:.4f}\")\n",
    "\n",
    "            print(\"\\nCalculating Confusion Matrix...\")\n",
    "            cm = confusion_matrix(y_true, y_pred_classes)\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=['Normal (0)', 'Abnormal (1)'],\n",
    "                        yticklabels=['Normal (0)', 'Abnormal (1)'])\n",
    "            plt.title('Confusion Matrix on Test Set')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.show()\n",
    "\n",
    "    except NameError as ne:\n",
    "        print(f\"\\nError during evaluation setup (NameError): {ne}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error re-building model, loading weights, or evaluating: {e}\")\n",
    "        print(\"Ensure all necessary variables (hyperparameters, paths) are defined and weights file is compatible.\")\n",
    "\n",
    "else:\n",
    "    print(f\"--- ERROR: Saved weights not found at {model_to_load_weights_from} ---\")\n",
    "    print(\"--- Please train the model first by running the preceding cells, or check MODEL_SAVE_PATH. ---\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 891819,
     "sourceId": 2332307,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7309746,
     "sourceId": 11833267,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 331656,
     "modelInstanceId": 311299,
     "sourceId": 376868,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2040.165009,
   "end_time": "2025-05-18T08:19:33.121282",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-18T07:45:32.956273",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
