{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bayesian Optimization for ResNet50 Architecture\n",
    "\n",
    "This notebook uses Optuna to find the optimal hyperparameters for three custom convolutional blocks appended to a pre-trained ResNet50 base model. The objective is to maximize the F1-score on the validation set for tuberculosis X-ray classification."
   ],
   "id": "5948504139d2d505"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setup and Imports",
   "id": "82ea842f1d69072e"
  },
  {
   "cell_type": "code",
   "id": "173ff53b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-14T11:25:49.297063Z",
     "iopub.status.busy": "2025-05-14T11:25:49.296172Z",
     "iopub.status.idle": "2025-05-14T11:26:05.238794Z",
     "shell.execute_reply": "2025-05-14T11:26:05.237984Z"
    },
    "papermill": {
     "duration": 15.947923,
     "end_time": "2025-05-14T11:26:05.239945",
     "exception": false,
     "start_time": "2025-05-14T11:25:49.292022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import ResNet50, InceptionV3, DenseNet121\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess_input\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback\n",
    "from glob import glob\n",
    "import json\n",
    "import shutil\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Optuna Version:\", optuna.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd23980b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:05.248348Z",
     "iopub.status.busy": "2025-05-14T11:26:05.247369Z",
     "iopub.status.idle": "2025-05-14T11:26:05.255961Z",
     "shell.execute_reply": "2025-05-14T11:26:05.255144Z"
    },
    "papermill": {
     "duration": 0.013604,
     "end_time": "2025-05-14T11:26:05.257096",
     "exception": false,
     "start_time": "2025-05-14T11:26:05.243492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Configuration\n",
    "MODEL_NAME = 'ResNet50'  # Options: 'ResNet50', 'InceptionV3', 'DenseNet121'\n",
    "\n",
    "# Pretrained Weights Configuration\n",
    "PRETRAINED_WEIGHTS_TYPE = 'radimagenet'\n",
    "\n",
    "\n",
    "RADIMAGENET_WEIGHTS_PATH_RESNET50 = '../../weights/RadImageNet-ResNet50_notop.h5'\n",
    "RADIMAGENET_WEIGHTS_PATH_INCEPTIONV3 = '../../weights/RadImageNet-InceptionV3_notop.h5'\n",
    "RADIMAGENET_WEIGHTS_PATH_DENSENET121 = '../../weights/RadImageNet-DenseNet121_notop.h5'\n",
    "\n",
    "\n",
    "# Fine-tuning Configuration\n",
    "UNFREEZE_AT_BLOCK = 'conv4_block1_out' # ResNet50: 'conv4_block1_out', DenseNet121: 'conv4_block1_concat', InceptionV3: 'mixed9'\n",
    "\n",
    "# Optuna Configuration\n",
    "OPTUNA_STUDY_NAME = f\"tb_bo_{MODEL_NAME}_{PRETRAINED_WEIGHTS_TYPE if PRETRAINED_WEIGHTS_TYPE else 'scratch'}\"\n",
    "BEST_PARAMS_FILE = f\"../../results/best_params_tb_{MODEL_NAME}_{PRETRAINED_WEIGHTS_TYPE if PRETRAINED_WEIGHTS_TYPE else 'scratch'}.json\"\n",
    "OPTUNA_DB_PATH = f\"sqlite:///../../results/optuna_study_tb_{MODEL_NAME}_{PRETRAINED_WEIGHTS_TYPE if PRETRAINED_WEIGHTS_TYPE else 'scratch'}.db\"\n",
    "OPTUNA_TRIALS = 1\n",
    "\n",
    "# Training Configuration\n",
    "IMG_SIZE = (224, 224)\n",
    "if MODEL_NAME == 'InceptionV3':\n",
    "    IMG_SIZE = (299, 299)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE_BO = 1e-5\n",
    "MAX_EPOCHS_BO = 1\n",
    "EARLY_STOPPING_PATIENCE_BO = 5\n",
    "\n",
    "# Data Configuration\n",
    "TB_DATASET_DIR_NAME = 'tuberculosis-tb-chest-xray-dataset'\n",
    "TB_SUBDIR = 'TB_Chest_Radiography_Database'\n",
    "TB_BASE_PATH = os.path.join('../../data/', TB_DATASET_DIR_NAME, TB_SUBDIR)\n",
    "TB_NORMAL_DIR = os.path.join(TB_BASE_PATH, 'Normal')\n",
    "TB_TUBERCULOSIS_DIR = os.path.join(TB_BASE_PATH, 'Tuberculosis')\n",
    "\n",
    "TEST_SPLIT_RATIO = 0.15\n",
    "VALID_SPLIT_RATIO = 0.15\n",
    "\n",
    "# Output\n",
    "print(\"--- Configuration ---\")\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Pretrained Weights Type: {PRETRAINED_WEIGHTS_TYPE}\")\n",
    "if PRETRAINED_WEIGHTS_TYPE == 'radimagenet':\n",
    "    if MODEL_NAME == 'ResNet50': print(f\"RadImageNet ResNet50 Path: {RADIMAGENET_WEIGHTS_PATH_RESNET50}\")\n",
    "    elif MODEL_NAME == 'InceptionV3': print(f\"RadImageNet InceptionV3 Path: {RADIMAGENET_WEIGHTS_PATH_INCEPTIONV3}\")\n",
    "    elif MODEL_NAME == 'DenseNet121': print(f\"RadImageNet DenseNet121 Path: {RADIMAGENET_WEIGHTS_PATH_DENSENET121}\")\n",
    "print(f\"Unfreeze at block: {UNFREEZE_AT_BLOCK if UNFREEZE_AT_BLOCK else ('Base Frozen' if PRETRAINED_WEIGHTS_TYPE else 'Fully Trainable (Scratch)')}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Optuna Study Name: {OPTUNA_STUDY_NAME}\")\n",
    "print(f\"Optuna DB Path: {OPTUNA_DB_PATH}\")\n",
    "print(f\"Best Params File: {BEST_PARAMS_FILE}\")\n",
    "print(f\"TB Dataset Path: {TB_BASE_PATH}\")\n",
    "print(\"--------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Data Loading and Preparation",
   "id": "dd75b6cc3fdafd4c"
  },
  {
   "cell_type": "code",
   "id": "16b481b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:05.292536Z",
     "iopub.status.busy": "2025-05-14T11:26:05.292311Z",
     "iopub.status.idle": "2025-05-14T11:26:10.539949Z",
     "shell.execute_reply": "2025-05-14T11:26:10.539119Z"
    },
    "papermill": {
     "duration": 5.253373,
     "end_time": "2025-05-14T11:26:10.541579",
     "exception": false,
     "start_time": "2025-05-14T11:26:05.288206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def load_tb_data():\n",
    "    tb_data_list = []\n",
    "    print(f\"Loading Tuberculosis data from: {TB_BASE_PATH}\")\n",
    "    try:\n",
    "        normal_files = glob(os.path.join(TB_NORMAL_DIR, '*.png'))\n",
    "        for f in normal_files: tb_data_list.append({'filepath': f, 'label': 0}) # 0 for Normal\n",
    "        print(f\"  Found {len(normal_files)} Normal images.\")\n",
    "\n",
    "        tuberculosis_files = glob(os.path.join(TB_TUBERCULOSIS_DIR, '*.png'))\n",
    "        for f in tuberculosis_files: tb_data_list.append({'filepath': f, 'label': 1}) # 1 for Tuberculosis\n",
    "        print(f\"  Found {len(tuberculosis_files)} Tuberculosis images.\")\n",
    "\n",
    "        if not tb_data_list:\n",
    "            print(\"WARNING: No TB images found. Check dataset paths.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df_tb = pd.DataFrame(tb_data_list)\n",
    "        print(f\"  Successfully processed {len(df_tb)} TB images in total.\")\n",
    "\n",
    "        original_count = len(df_tb)\n",
    "        df_tb = df_tb[df_tb['filepath'].apply(os.path.exists)]\n",
    "        files_removed = original_count - len(df_tb)\n",
    "        if files_removed > 0:\n",
    "            print(f\"WARNING: Removed {files_removed} non-existent file entries from TB data.\")\n",
    "\n",
    "        if df_tb.empty:\n",
    "            print(\"ERROR: No valid TB image files found after checking paths.\")\n",
    "        return df_tb\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading TB data: {e}. Check paths and file structure.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df_tb_all = load_tb_data()\n",
    "\n",
    "if not df_tb_all.empty:\n",
    "    print(f\"\\nTotal TB images loaded: {len(df_tb_all)}\")\n",
    "    print(\"TB Label distribution:\\n\", df_tb_all['label'].value_counts())\n",
    "\n",
    "    # Splitting data into training, validation, and test sets\n",
    "    if len(df_tb_all) > 1 and df_tb_all['label'].nunique() > 1:\n",
    "        # Split off test set\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            df_tb_all,\n",
    "            test_size=TEST_SPLIT_RATIO,\n",
    "            stratify=df_tb_all['label'],\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        adjusted_valid_split_ratio = VALID_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO) if (1 - TEST_SPLIT_RATIO) > 0 else 0\n",
    "        if len(train_val_df) > 1 and train_val_df['label'].nunique() > 1 and adjusted_valid_split_ratio > 0:\n",
    "            train_df, val_df = train_test_split(\n",
    "                train_val_df,\n",
    "                test_size=adjusted_valid_split_ratio,\n",
    "                stratify=train_val_df['label'],\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            train_df = train_val_df\n",
    "            val_df = pd.DataFrame(columns=train_val_df.columns)\n",
    "            if adjusted_valid_split_ratio > 0:\n",
    "                 print(\"Warning: Could not perform validation split properly due to insufficient data or classes after initial split. Validation set might be small or empty.\")\n",
    "\n",
    "        print(f\"\\nData Split:\")\n",
    "        print(f\"  Training samples: {len(train_df)}\")\n",
    "        print(f\"  Validation samples: {len(val_df)}\")\n",
    "        print(f\"  Test samples: {len(test_df)}\")\n",
    "\n",
    "        if not train_df.empty: print(\"Train label distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
    "        if not val_df.empty: print(\"Validation label distribution:\\n\", val_df['label'].value_counts(normalize=True))\n",
    "        if not test_df.empty: print(\"Test label distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
    "\n",
    "        if train_df.empty or val_df.empty:\n",
    "            print(\"ERROR: Training or Validation DataFrame is empty after splitting. BO cannot proceed.\")\n",
    "    else:\n",
    "        print(\"ERROR: Not enough data or classes to perform stratified split for TB dataset.\")\n",
    "        train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "else:\n",
    "    print(\"ERROR: TB DataFrame is empty. Cannot proceed with data splitting.\")\n",
    "    train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Calculate Class Weights\n",
    "class_weights_dict = None\n",
    "if not train_df.empty:\n",
    "    try:\n",
    "        y_train_labels = train_df['label'].astype(int).values\n",
    "        \n",
    "        unique_classes = np.unique(y_train_labels)\n",
    "        if len(unique_classes) > 1:\n",
    "            weights = class_weight.compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=unique_classes,\n",
    "                y=y_train_labels\n",
    "            )\n",
    "            class_weights_dict = dict(zip(unique_classes, weights))\n",
    "            print(f\"\\n--- Class Weights Calculated ---\")\n",
    "            print(f\"Class weights to be used: {class_weights_dict}\")\n",
    "            print(f\"Applied to classes: {unique_classes}\")\n",
    "            print(\"------------------------------\")\n",
    "        else:\n",
    "            print(\"\\nWarning: Only one class found in training data. Cannot calculate class weights.\")\n",
    "            class_weights_dict = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError calculating class weights: {e}\")\n",
    "        class_weights_dict = None\n",
    "else:\n",
    "    print(\"\\nWarning: train_df is empty. Skipping class weight calculation.\")\n",
    "    class_weights_dict = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Data Augmentation and Generators",
   "id": "b6f6183c5c87c7e0"
  },
  {
   "cell_type": "code",
   "id": "7847de13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:10.560369Z",
     "iopub.status.busy": "2025-05-14T11:26:10.560066Z",
     "iopub.status.idle": "2025-05-14T11:26:11.728783Z",
     "shell.execute_reply": "2025-05-14T11:26:11.728164Z"
    },
    "papermill": {
     "duration": 1.176036,
     "end_time": "2025-05-14T11:26:11.729821",
     "exception": false,
     "start_time": "2025-05-14T11:26:10.553785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "train_generator = None\n",
    "validation_generator = None\n",
    "test_generator = None\n",
    "\n",
    "# Select the correct preprocessing function based on MODEL_NAME\n",
    "if MODEL_NAME == 'ResNet50':\n",
    "    preprocess_input_func = resnet_preprocess_input\n",
    "elif MODEL_NAME == 'InceptionV3':\n",
    "    preprocess_input_func = inception_preprocess_input\n",
    "elif MODEL_NAME == 'DenseNet121':\n",
    "    preprocess_input_func = densenet_preprocess_input\n",
    "else:\n",
    "    print(f\"WARNING: Preprocessing function not explicitly set for {MODEL_NAME}. Using generic rescaling.\")\n",
    "    preprocess_input_func = lambda x: x / 255.0\n",
    "\n",
    "if not train_df.empty and not val_df.empty:\n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input_func, # Apply model-specific preprocessing\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='filepath',\n",
    "        y_col='label',\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='raw',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    if hasattr(train_generator, 'samples'):\n",
    "        num_train_classes = train_df['label'].nunique()\n",
    "        print(f\"Train generator created. Found {train_generator.samples} images belonging to {num_train_classes} classes.\")\n",
    "    else:\n",
    "        print(\"Train generator creation failed or train_df is empty.\")\n",
    "\n",
    "\n",
    "    # Validation data generator\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input_func\n",
    "    )\n",
    "    \n",
    "    validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='filepath',\n",
    "        y_col='label',\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='raw',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    if hasattr(validation_generator, 'samples'): \n",
    "        num_val_classes = val_df['label'].nunique() \n",
    "        print(f\"Validation generator created. Found {validation_generator.samples} images belonging to {num_val_classes} classes.\")\n",
    "    else:\n",
    "        print(\"Validation generator creation failed or val_df is empty.\")\n",
    "\n",
    "\n",
    "    if not test_df.empty:\n",
    "        test_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_input_func\n",
    "        )\n",
    "        test_generator = test_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            x_col='filepath',\n",
    "            y_col='label',\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='raw',\n",
    "            shuffle=False\n",
    "        )\n",
    "        if hasattr(test_generator, 'samples'):\n",
    "            num_test_classes = test_df['label'].nunique()\n",
    "            print(f\"Test generator created. Found {test_generator.samples} images belonging to {num_test_classes} classes.\")\n",
    "        else:\n",
    "            print(\"Test generator creation failed or test_df is empty.\")\n",
    "else:\n",
    "    print(\"Skipping generator creation as train_df or val_df is empty.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Architecture Definition",
   "id": "de61e279698a96cb"
  },
  {
   "cell_type": "code",
   "id": "08623cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:11.738579Z",
     "iopub.status.busy": "2025-05-14T11:26:11.738361Z",
     "iopub.status.idle": "2025-05-14T11:26:11.751527Z",
     "shell.execute_reply": "2025-05-14T11:26:11.750895Z"
    },
    "papermill": {
     "duration": 0.018775,
     "end_time": "2025-05-14T11:26:11.752537",
     "exception": false,
     "start_time": "2025-05-14T11:26:11.733762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def build_model_for_bo(hp_filters1, hp_kernel1_str, hp_pool1,\n",
    "                       hp_filters2, hp_kernel2_str, hp_pool2,\n",
    "                       hp_filters3, hp_kernel3_str, hp_pool3,\n",
    "                       model_name_local, pretrained_weights_type_local, unfreeze_at_local):\n",
    "    def parse_kernel_size(k_str): return tuple(map(int, k_str.split('x')))\n",
    "    kernel1_tuple = parse_kernel_size(hp_kernel1_str)\n",
    "    kernel2_tuple = parse_kernel_size(hp_kernel2_str)\n",
    "    kernel3_tuple = parse_kernel_size(hp_kernel3_str)\n",
    "\n",
    "    inputs = keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3), name=\"input_image\")\n",
    "    base_model_instance = None\n",
    "    keras_weights_arg = None\n",
    "    custom_weights_path = None\n",
    "\n",
    "    if pretrained_weights_type_local == 'imagenet':\n",
    "        keras_weights_arg = 'imagenet'\n",
    "    elif pretrained_weights_type_local == 'radimagenet':\n",
    "        keras_weights_arg = None\n",
    "        if model_name_local == 'ResNet50': custom_weights_path = RADIMAGENET_WEIGHTS_PATH_RESNET50\n",
    "        elif model_name_local == 'InceptionV3': custom_weights_path = RADIMAGENET_WEIGHTS_PATH_INCEPTIONV3\n",
    "        elif model_name_local == 'DenseNet121': custom_weights_path = RADIMAGENET_WEIGHTS_PATH_DENSENET121\n",
    "        else: print(f\"Warning: RadImageNet path not specified for {model_name_local}\")\n",
    "    elif pretrained_weights_type_local is None:\n",
    "        keras_weights_arg = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pretrained_weights_type: {pretrained_weights_type_local}\")\n",
    "\n",
    "\n",
    "    if model_name_local == 'ResNet50': base_model_func = ResNet50\n",
    "    elif model_name_local == 'InceptionV3': base_model_func = InceptionV3\n",
    "    elif model_name_local == 'DenseNet121': base_model_func = DenseNet121\n",
    "    else: raise ValueError(f\"Unsupported model_name: {model_name_local}\")\n",
    "\n",
    "    print(f\"Loading base model: {model_name_local} with Keras weights arg: {keras_weights_arg}\")\n",
    "    base_model_instance = base_model_func(\n",
    "        include_top=False, weights=keras_weights_arg,\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "    )\n",
    "\n",
    "    if pretrained_weights_type_local == 'radimagenet' and custom_weights_path:\n",
    "        if os.path.exists(custom_weights_path):\n",
    "            print(f\"Loading custom RadImageNet weights for {model_name_local} from: {custom_weights_path}\")\n",
    "            try:\n",
    "                base_model_instance.load_weights(custom_weights_path, by_name=True, skip_mismatch=True)\n",
    "                print(\"Custom RadImageNet weights loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR loading custom RadImageNet weights: {e}. Model will use initial weights (random if Keras weights_arg was None).\")\n",
    "        else:\n",
    "            print(f\"WARNING: RadImageNet weight file not found at {custom_weights_path}. Model will use initial weights.\")\n",
    "    elif pretrained_weights_type_local == 'radimagenet' and not custom_weights_path:\n",
    "        print(f\"WARNING: PRETRAINED_WEIGHTS_TYPE is 'radimagenet' but no path specified for {model_name_local}. Model will use initial weights (random if Keras weights_arg was None).\")\n",
    "\n",
    "\n",
    "    # Fine-tuning / Freezing\n",
    "    if pretrained_weights_type_local is None:\n",
    "        print(f\"Base model {model_name_local} will be trained from scratch.\")\n",
    "        base_model_instance.trainable = True\n",
    "    elif unfreeze_at_local and unfreeze_at_local.lower() == 'full_unfreeze':\n",
    "        print(f\"Fine-tuning enabled: Unfreezing all layers of the base model {model_name_local}.\")\n",
    "        base_model_instance.trainable = True\n",
    "    elif unfreeze_at_local:\n",
    "        print(f\"Fine-tuning enabled: Attempting to unfreeze layers from '{unfreeze_at_local}' onwards for {model_name_local}.\")\n",
    "        base_model_instance.trainable = True\n",
    "        set_trainable = False\n",
    "        for layer in base_model_instance.layers:\n",
    "            if unfreeze_at_local in layer.name: set_trainable = True\n",
    "            layer.trainable = set_trainable\n",
    "        if not any(layer.trainable for layer in base_model_instance.layers if unfreeze_at_local in layer.name and set_trainable):\n",
    "             print(f\"WARNING: Unfreeze point '{unfreeze_at_local}' not found or did not result in trainable layers as expected. Check layer names. Base model might remain frozen or fully trainable depending on initial state.\")\n",
    "    else: # Default: freeze the base model if pretrained weights are used and no unfreezing specified\n",
    "        print(f\"Keeping the base model {model_name_local} frozen.\")\n",
    "        base_model_instance.trainable = False\n",
    "\n",
    "    x = base_model_instance(inputs, training=base_model_instance.trainable)\n",
    "\n",
    "    def add_conv_pool_block(x_input, filters, kernel_size, pool_type, block_name):\n",
    "        pool_func = layers.MaxPooling2D if pool_type == 'max' else layers.AveragePooling2D\n",
    "        x_out = layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', activation='relu', name=f'custom_{block_name}_conv')(x_input)\n",
    "        if x_out.shape[1] > 1 and x_out.shape[2] > 1:\n",
    "            x_out = pool_func(pool_size=(2, 2), strides=2, name=f'custom_{block_name}_pool')(x_out)\n",
    "        else: print(f\"Skipping Pooling for {block_name} (Input shape to pool: {x_out.shape})\")\n",
    "        return x_out\n",
    "\n",
    "    x = add_conv_pool_block(x, hp_filters1, kernel1_tuple, hp_pool1, \"block1\")\n",
    "    x = add_conv_pool_block(x, hp_filters2, kernel2_tuple, hp_pool2, \"block2\")\n",
    "    x = add_conv_pool_block(x, hp_filters3, kernel3_tuple, hp_pool3, \"block3\")\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name='custom_gap')(x)\n",
    "    x = layers.Dropout(0.5, name='custom_dropout')(x)\n",
    "    x = layers.Dense(256, activation='relu', name='custom_dense_256')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name='output_sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    print(f\"Compiling model with learning rate: {LEARNING_RATE_BO}\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_BO),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11c38517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:11.760103Z",
     "iopub.status.busy": "2025-05-14T11:26:11.759917Z",
     "iopub.status.idle": "2025-05-14T11:26:16.649338Z",
     "shell.execute_reply": "2025-05-14T11:26:16.648497Z"
    },
    "papermill": {
     "duration": 4.894799,
     "end_time": "2025-05-14T11:26:16.650805",
     "exception": false,
     "start_time": "2025-05-14T11:26:11.756006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Summary for Verification\n",
    "\n",
    "print(f\"--- Building a temporary model for {MODEL_NAME} to display summary ---\")\n",
    "print(f\"Using PRETRAINED_WEIGHTS_TYPE: '{PRETRAINED_WEIGHTS_TYPE}', UNFREEZE_AT_BLOCK: '{UNFREEZE_AT_BLOCK}'\")\n",
    "\n",
    "# Dummy hyperparameters for the custom top layers\n",
    "dummy_hp = {\n",
    "    'hp_filters1': 32, 'hp_kernel1_str': '3x3', 'hp_pool1': 'max',\n",
    "    'hp_filters2': 64, 'hp_kernel2_str': '3x3', 'hp_pool2': 'max',\n",
    "    'hp_filters3': 128, 'hp_kernel3_str': '3x3', 'hp_pool3': 'max'\n",
    "}\n",
    "\n",
    "# Build a temporary model instance\n",
    "temp_model = build_model_for_bo(\n",
    "    **dummy_hp, # Unpack dummy hyperparameters\n",
    "    model_name_local=MODEL_NAME,\n",
    "    pretrained_weights_type_local=PRETRAINED_WEIGHTS_TYPE,\n",
    "    unfreeze_at_local=UNFREEZE_AT_BLOCK\n",
    ")\n",
    "\n",
    "print(\"\\nMODEL SUMMARY\")\n",
    "temp_model.summary()\n",
    "\n",
    "# Clean up the temporary model and clear session\n",
    "del temp_model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\n--- End of Summary Cell ---\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Bayesian Optimization Objective Function",
   "id": "ea62a2fda11cb831"
  },
  {
   "cell_type": "code",
   "id": "36de6479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:16.663645Z",
     "iopub.status.busy": "2025-05-14T11:26:16.663379Z",
     "iopub.status.idle": "2025-05-14T11:26:16.680118Z",
     "shell.execute_reply": "2025-05-14T11:26:16.679591Z"
    },
    "papermill": {
     "duration": 0.024042,
     "end_time": "2025-05-14T11:26:16.681071",
     "exception": false,
     "start_time": "2025-05-14T11:26:16.657029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def objective(trial):\n",
    "    global MODEL_NAME, PRETRAINED_WEIGHTS_TYPE, UNFREEZE_AT_BLOCK\n",
    "    global class_weights_dict\n",
    "    \n",
    "    if not train_generator or not validation_generator or len(train_generator) == 0 or len(validation_generator) == 0:\n",
    "        print(f\"TRIAL {trial.number} SKIPPED: Invalid data generator(s).\")\n",
    "        raise optuna.TrialPruned(\"Data generators not available or empty.\")\n",
    "\n",
    "    # Hyperparameter suggestions\n",
    "    filters1 = trial.suggest_int('filters1', 32, 128, step=16)\n",
    "    kernel1_str = trial.suggest_categorical('kernel1', ['3x3', '5x5', '7x7'])\n",
    "    pool1 = trial.suggest_categorical('pool1', ['max', 'average'])\n",
    "    filters2 = trial.suggest_int('filters2', 64, 256, step=32)\n",
    "    kernel2_str = trial.suggest_categorical('kernel2', ['3x3', '5x5', '7x7'])\n",
    "    pool2 = trial.suggest_categorical('pool2', ['max', 'average'])\n",
    "    filters3 = trial.suggest_int('filters3', 128, 512, step=64)\n",
    "    kernel3_str = trial.suggest_categorical('kernel3', ['3x3', '5x5', '7x7'])\n",
    "    pool3 = trial.suggest_categorical('pool3', ['max', 'average'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Build the model\n",
    "    try:\n",
    "        model = build_model_for_bo(\n",
    "            filters1, kernel1_str, pool1,\n",
    "            filters2, kernel2_str, pool2,\n",
    "            filters3, kernel3_str, pool3,\n",
    "            MODEL_NAME, PRETRAINED_WEIGHTS_TYPE, UNFREEZE_AT_BLOCK\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR building model in Trial {trial.number} for {MODEL_NAME} ({PRETRAINED_WEIGHTS_TYPE}): {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=EARLY_STOPPING_PATIENCE_BO, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1\n",
    "    )\n",
    "    pruning_callback = KerasPruningCallback(trial=trial, monitor='val_loss', interval=1)\n",
    "\n",
    "    print(f\"\\n--- Optuna Trial {trial.number} for {MODEL_NAME} ({PRETRAINED_WEIGHTS_TYPE}) ---\")\n",
    "    print(f\"Params: F1={filters1}, K1={kernel1_str}, P1={pool1} | F2={filters2}, K2={kernel2_str}, P2={pool2} | F3={filters3}, K3={kernel3_str}, P3={pool3}\")\n",
    "\n",
    "    current_class_weight = None\n",
    "    if class_weights_dict is not None:\n",
    "        current_class_weight = class_weights_dict\n",
    "        print(f\"Using class weights: {current_class_weight}\")\n",
    "    else:\n",
    "        print(\"Warning: class_weights_dict is None. Training without class weights.\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            train_generator, \n",
    "            epochs=MAX_EPOCHS_BO, \n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[early_stopping, pruning_callback],\n",
    "            class_weight=current_class_weight,  # Apply class weighting\n",
    "            verbose=1\n",
    "        )\n",
    "    except optuna.TrialPruned as e:\n",
    "        print(f\"Trial {trial.number} pruned by Optuna: {e}\")\n",
    "        raise e \n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(f\"ERROR in Trial {trial.number} - ResourceExhaustedError: {e}. Pruning.\")\n",
    "        raise optuna.TrialPruned(\"ResourceExhaustedError during training.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during model.fit in Trial {trial.number}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Evaluating Trial {trial.number} on validation set...\")\n",
    "    try:\n",
    "        val_loss, val_acc, val_prec, val_rec = model.evaluate(validation_generator, verbose=0)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during model.evaluate in Trial {trial.number}: {e}\")\n",
    "        return 0.0 # Return low F1-score for evaluation errors\n",
    "\n",
    "    # Calculate F1-score\n",
    "    val_f1 = 0.0\n",
    "    if (val_prec + val_rec) > 0:\n",
    "        val_f1 = 2 * (val_prec * val_rec) / (val_prec + val_rec)\n",
    "    else:\n",
    "        print(f\"Warning: Precision + Recall = 0 for Trial {trial.number}. F1-score will be 0.\")\n",
    "    \n",
    "    print(f\"Trial {trial.number} ({MODEL_NAME}, {PRETRAINED_WEIGHTS_TYPE}) Results: Val Loss={val_loss:.4f}, Acc={val_acc:.4f}, Prec={val_prec:.4f}, Rec={val_rec:.4f}, F1={val_f1:.4f}\")\n",
    "    \n",
    "    return val_f1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Execute Optuna Study",
   "id": "21d94452f7c19319"
  },
  {
   "cell_type": "code",
   "id": "d5d6ec9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:26:16.690352Z",
     "iopub.status.busy": "2025-05-14T11:26:16.690140Z",
     "iopub.status.idle": "2025-05-14T20:43:09.522741Z",
     "shell.execute_reply": "2025-05-14T20:43:09.521888Z"
    },
    "papermill": {
     "duration": 33412.838796,
     "end_time": "2025-05-14T20:43:09.524046",
     "exception": false,
     "start_time": "2025-05-14T11:26:16.685250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "study = None\n",
    "best_trial_info = None\n",
    "\n",
    "if train_generator and validation_generator:\n",
    "    print(f\"\\n--- Starting Optuna Study for {MODEL_NAME} with {PRETRAINED_WEIGHTS_TYPE if PRETRAINED_WEIGHTS_TYPE else 'scratch'} weights ---\")\n",
    "    try:\n",
    "        study = optuna.create_study(\n",
    "            study_name=OPTUNA_STUDY_NAME,\n",
    "            direction='maximize',\n",
    "            storage=OPTUNA_DB_PATH,\n",
    "            load_if_exists=True,\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_startup_trials=5, # Allow first few trials to complete regardless of intermediate values\n",
    "                n_warmup_steps=EARLY_STOPPING_PATIENCE_BO + 2, # Don't prune before this many epochs\n",
    "                interval_steps=1 # Check for pruning every epoch after warmup\n",
    "            )\n",
    "        )\n",
    "\n",
    "        n_completed_trials_total = len(study.trials)\n",
    "        n_finished_trials = len([t for t in study.trials if t.state in [\n",
    "            optuna.trial.TrialState.COMPLETE,\n",
    "            optuna.trial.TrialState.PRUNED,\n",
    "            optuna.trial.TrialState.FAIL\n",
    "        ]])\n",
    "\n",
    "        n_trials_to_run = OPTUNA_TRIALS - n_finished_trials\n",
    "\n",
    "        if n_trials_to_run > 0:\n",
    "            print(f\"Study '{study.study_name}' has {n_finished_trials} finished (Complete/Pruned/Fail) trials.\")\n",
    "            print(f\"Running {n_trials_to_run} more trials (target total: {OPTUNA_TRIALS}).\")\n",
    "\n",
    "            timeout_seconds = 11 * 3600 # Time limit 11 hours because Kaggle session limit is 12 hours\n",
    "            print(f\"Optuna study timeout set to: {timeout_seconds} seconds ({timeout_seconds/3600:.1f} hours)\")\n",
    "\n",
    "            study.optimize(\n",
    "                objective,\n",
    "                n_trials=n_trials_to_run,\n",
    "                timeout=timeout_seconds,\n",
    "                callbacks=[lambda current_study, current_trial: tf.keras.backend.clear_session()]\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Study '{study.study_name}' already has {n_finished_trials} finished trials (target was {OPTUNA_TRIALS}). No new trials will be run.\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- Optuna Study Finished ---\")\n",
    "        print(f\"Study Name: {study.study_name}\")\n",
    "        print(f\"Total trials in study database: {len(study.trials)}\")\n",
    "        completed_trials_list = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        print(f\"  Completed: {len(completed_trials_list)}\")\n",
    "        print(f\"  Pruned: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "        print(f\"  Failed: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "        print(f\"  Running: {len([t for t in study.trials if t.state == optuna.trial.TrialState.RUNNING])}\")\n",
    "\n",
    "        if completed_trials_list:\n",
    "            best_trial_info = study.best_trial\n",
    "            print(f\"\\nBest completed trial for {MODEL_NAME} ({PRETRAINED_WEIGHTS_TYPE if PRETRAINED_WEIGHTS_TYPE else 'scratch'}):\")\n",
    "            print(f\"  Number: {best_trial_info.number}\")\n",
    "            print(f\"  Best F1-score (validation): {best_trial_info.value:.4f}\")\n",
    "            print(f\"  Best hyperparameters: {best_trial_info.params}\")\n",
    "\n",
    "            # Save Best Hyperparameters\n",
    "            print(f\"\\nSaving best hyperparameters for {MODEL_NAME} to {BEST_PARAMS_FILE}\")\n",
    "            try:\n",
    "                params_to_save = {k: (list(v) if isinstance(v, tuple) else v) for k, v in best_trial_info.params.items()}\n",
    "                with open(BEST_PARAMS_FILE, 'w') as f:\n",
    "                    json.dump(params_to_save, f, indent=4)\n",
    "                print(\"Best hyperparameters saved successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR saving hyperparameters: {e}\")\n",
    "        else:\n",
    "            print(f\"No trials completed successfully for {MODEL_NAME} ({PRETRAINED_WEIGHTS_TYPE if PRETRAINED_WEIGHTS_TYPE else 'scratch'}). Cannot determine best trial.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the Optuna study for {MODEL_NAME}: {e}\")\n",
    "else:\n",
    "    print(\"\\n--- Optuna Study Skipped ---\")\n",
    "    if not train_generator: print(\"Reason: Training data generator was not created or is empty.\")\n",
    "    if not validation_generator: print(\"Reason: Validation data generator was not created or is empty.\")\n",
    "    print(f\"Cannot run Optuna study for {MODEL_NAME}.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 891819,
     "sourceId": 2332307,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7412552,
     "sourceId": 11803486,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 331656,
     "modelInstanceId": 311299,
     "sourceId": 376868,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33455.567067,
   "end_time": "2025-05-14T20:43:16.220656",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-14T11:25:40.653589",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
